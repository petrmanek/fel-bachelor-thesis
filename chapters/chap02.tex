\chapter{Data Structure and Storage}
In this chapter, a data scheme capable of archiving Timepix footage for longer time periods is proposed. The primary concern is to minimize access latency in queries based on the time of measurement and the device of origin. For that reason, several auxiliary data structures, such as the index database, are introduced.

\section{Formal Requirements}
Requirements on such a data structure are as common as database requirements can get. It should be a reliable permanent storage element, accessible for reading from multiple workstations at a same time and robust enough to withstand minor hardware failures. With 15 detectors already installed at ATLAS, and possible option of installing another 5, the database should be designed to hold frames from up to 20 Timepix devices for the entire expected time period of their operation at LHC (that from June 2015 to LS3 in 2021).
% ZKRATKA: LHC
% ZKRATKA: LS

\todo
% CITACE: LHC schedule, LS2, LS3

As more and more frames arrive from the detector network, our database should allow to be periodically extended with new data, possibly processing and converting pixel matrices into cluster lists, as described in the previous sections. Since the database will be primary storage site for all research data, there should be multiple independent copies of it as backups and the database structure should be designed with logic to enable timely synchronization of these copies.

Apart from all the requirements already listed, it is important mention that the anticipated structure of the majority of user queries is known. With regards to this information, data storage and retrieval procedures may be optimized to accelerate such queries. It was determined that the most queries are going to filter data by time or by device. This is indeed a very natural approach, provided that every device in the network is positioned and oriented in way allowing only for a certain type of particles to be observed. Researchers looking for signs of specific particles might often request data based on other experiments, which were conducted in a determinate time period and involved only a specific group of detectors in the network.

\section{Database}
\label{db:definition}
In this section, the Timepix footage database is defined. Accounting for the ever-growing nature of our data, the database is separated into two parts. The first part is to contain data which has already been processed by the cluster analysis, and is ready to be accessed by users. The second part is to contain data which has arrived from CERN in its raw form but hasn't been processed yet. As one might observe, this separation of data serves a fundamental purpose, that is to distinguish intermediate products from finished ones.

% ZKRATKA: UNIX
% ZKRATKA: HTTP
% ZKRATKA: SMB
% ZKRATKA: SSH
% ZKRATKA: AFP
% ZKRATKA: FTP
% ZKRATKA: EOS
% CITACE: UNIX file system

\subsection{Definition}
For the sake of compatibility, database is based on a UNIX file system. This approach enables many users, not necessarily only those using UNIX-based operating systems, to access it directly by means of widely-used and standardized protocols, such as FTP, SMB, SSH, AFP or HTTP\label{db:supported-protocols}. Utilization of these protocols contributes not only to the universality of the database, it also resolves shared resource access and other data concurrency issues. Some of these features may prove to be useful later on when synchronizing various storage sites in order to back up or restore data. In addition, UNIX file systems also offer fundamental security features, allowing administrators to grant read-write privileges to a certain group of users, while limiting others to a mere read-only access.

\todo
% CITACE: Označení TPX detektorů v ATLASu.

The file system has two directories named \texttt{processed} and \texttt{downloading}, corresponding to the respective sections of the database. In these directories, data is further grouped in subdirectories by the device of origin. To make navigation easier, device directory names use numeric identifiers in compliance with already published literature. For instance, all data originating from the detector no. 7 would be stored in a directory named \texttt{ATPX07}. In such directory, footage would be stored in time-coded files (or directories, should multiple files be grouped under single time code) according to the naming pattern: \texttt{[yyyy]\_[mm]\_[dd]\_ATPX[id]} (where \texttt{[id]} represents the device identifier and \texttt{[yyyy]}, \texttt{[mm]}, \texttt{[dd]} represent year, month and day of acquisition respectively).

If it is not practical to group files by the day of acquisition, footage can be grouped by an alternative naming pattern with hourly granularity: \texttt{[yyyy]\_[mm]\_[dd]\_ATPX[id]\_[hh]} (where \texttt{[hh]} represents the hour of acquisition and other entities are treated as in the previous pattern). Note that in spite of grouping data files in separate subdirectories by the device of origin, the device identifier is intentionally included in the naming pattern for reasons of redundancy.

The directory structure described so far satisfies all requirements from the previous section. What's more, it optimizes access to data generated from specific devices at specific times, so that the majority of user requests is handled in timely manner.

All data files are stored at the lowest level of the directory structure under time-coded names according to our naming patterns (see Figure \ref{fig:db-structure}). Should more files fall under the same time code (marginal scenario), they are to be grouped in a directory with a time-coded name. File structure in such a directory is undefined. Although it is not required, it is expected that files in the \texttt{processed} directory are encoded in the ROOT format, whereas file in the \texttt{downloading} directory are encoded in plain text, as that is the initial format of all unprocessed footage. Apart from these two formats, files of different types will be tolerated, but regarded as secondary.

\begin{figure}[t]
\begin{center}

\begin{tikzpicture}[
    every node/.style={
        draw=black,
        thick,
        anchor=west,
        inner sep=2pt,
        minimum size=1pt,
    },
    grow via three points={
        one child at (0.8,-0.7) and two children at (0.8,-0.7) and (0.8,-1.4)
    },
    edge from parent path={
        ($(\tikzparentnode\tikzparentanchor)+(.4cm,0pt)$) |- (\tikzchildnode\tikzchildanchor)
    },
    growth parent anchor=west,
    parent anchor=south west
  ]
  \node {\textbf{Timepix Database/}}
    child { node {\texttt{downloading/}} }
    child { node {\texttt{processed/}}
    	child { node [draw=none] {\texttt{README.txt}} }
    	child { node {\texttt{ATPX01/}} }
    	child { node {\texttt{ATPX02/}}
    		child { node [draw=none] {\ldots}}
    		child { node [draw=none] {\texttt{2015\_08\_24\_ATPX02.root}} }
    		child { node [draw=none] {\texttt{2015\_08\_25\_ATPX02.root}} }
    		child { node {\texttt{2015\_08\_26\_ATPX02/}}
    			child { node [draw=none] {\texttt{morning.root}} }
    			child { node [draw=none] {\texttt{afternoon.root}} }
    			child { node [draw=none] {\texttt{evening.root}} }
    		}
    	}
    };
\end{tikzpicture}

\caption{Example of the database file system structure.}
\label{fig:db-structure}
\end{center}
\end{figure}

% ZKRATKA: zip
% ZKRATKA: tar

To preserve storage space, various compression methods. Supported compression formats are ZIP, GZIP and TAR, or any combination of them. As individual data files are expected to grow quite large in size, compression is to be utilized only at the lowest level of the directory structure, that is in the time-coded data files and directories. Individual archives are allowed to store at most one time-coded file (or directory), thus being able to overtake file's time-coded name while remaining unique in the file listing. It is preferred but not required that all data files stored in a single directory are either all compressed, or none of them is, as any deviation from this scheme might point to an incomplete or broken data transaction.

Lastly, it is forbidden to store any recursively compressed structures (such as archive within other archive, etc.). The recommended alternative is to increase compression level in already existing archives instead of constructing new ones. Note that this rule also applies for all data formats which use compression inherently, such as ROOT.

\todo
%%% Benedikt recommendation modifications finished up to this point

\subsection{Expected Volume of Data}
With our definition in mind, we will now perform a simple extrapolation to obtain an upper bound on the size of our database.

Assuming that one hour of footage stored in the multi-frame format may take up to 4~gigabytes in size (depending on the frequency of acquisition), we have 96~gigabytes per sensor per day. Accounting for the longest possible time of operation, our database will store up to 2,437~days of footage simultaneously recorded by up to 20~detectors. That means that our database will have to hold about 4.7~petabytes worth of uncompressed information. If we use Collin's compression algorithm benchmark from \cite{GzipBenchmark} as baseline, it is possible to estimate that a common variant of GZIP algorithm will reduce the file size in average by 75.9\%. Applying this compression on our multi-frame data files, our database would have to hold \textit{only} about 1.1 petabyte of archives.

We will now perform analogous calculation for the ROOT file format. Since the file structure already utilizes its own proprietary compression algorithms, we expect the overall volume to decrease significantly in comparison with the raw uncompressed multi-frame data. From the data recorded by the ATLAS Timepix network in the fall of 2015, we observe that a single day of footage stored in the ROOT format may take up to 18~gigabytes in size. Using the same constants as before, we arrive at the conclusion that our database will have to hold about 877~terabytes of information. This result is in agreement with our expectations.

Please note that neither of these upper bounds is by any means, since we intentionally over-estimated the number of detectors in our network and the length of the operation period in our assumptions. In addition, it is likely that some of our detectors will be configured to capture data with frequencies lower than the maximum possible frequency as every device is configured separately to observe particles at different speeds. For all these reasons, our estimation only gives us vague information about the orders of magnitude of storage space required to operate our database and its subsequent backups. In spite of this limitation, the estimation suffices to design and rate other components of our system. 


\section{Index Database}
So far, we have established a set of rules for our file system, in order to quickly obtain data from a specific device captured at a specific time. These facilities are sufficient for navigating and accessing data in rudimentary manner, but are certainly not optimal. For instance, our rules do not define any conventions regarding retrieval of specific frames from files in the ROOT data format. Due to this limitation, users seeking individual frames will have to download bulks of data from longer time periods (their length can vary from an hour to a day in time and from hundreds of megabytes to several gigabytes in size), which may induce unnecessary processing overhead and memory shortages.

There is also no guarantee that time-coded nodes in our directory structure will be individual files. If such nodes happen to be directories, the file structure inside of such directories is undefined, and may require additional decisions on the user side. And what do we do when we want to retrieve frames based on different criteria than time and device of origin? At the moment, we have no option other than to directly enumerate frames stored in all files in our file system, which (considering their potential size) might not be a preferrable solution. To resolve all these issues, we will introduce one more element to our design---an index database.

This database will be contain information which can be recalculated at any instant from the primary data files, hence it will not need to be backed up. The information stored in our database will mostly include, as the name suggests, index of all files and frames on the record and addresses pointing to the them on our file system. In addition, the index database will also store some commonly requested aggregated values.



\subsection{Definition}
% ZKRATKA: SQL
The index database will be compliant to the SQL standard, so that users may design their own queries. For the reasons of simplicity, we will define only three basic entities in our database. The relationships between these entities are depicted in Figure \ref{db:index-uml}, whereas the meaning of their members is defined in this section.

% OBR: UML diagram databáze

\label{db:definition}
\begin{description}
	\item[Sensor]
	Sensor represents a single Timepix device, from which data can be acquired. For full definition of the SQL table, see Listing \ref{lst:sql-sensors}.

	\begin{description}
		\item[Sensor Identifier (\texttt{sid})]
		Identifier of the device, unique within the index database.

		\item[Name (\texttt{name})]
		Readable name of the sensor, consistent with the other literature.
		% CITACE: označení TPX detektorů v ATLASu

		\item[Calibration Constants (\texttt{calibration\_layer1}, \texttt{calibration\_layer2})]
		Constants used for luminosity calculation, available only for some devices.
	\end{description}

	\item[ROOT File]
	File represents a single file in the ROOT data format, containing data acquired from a single Timepix device in a determinate time period. For full definition of the SQL table, see Listing \ref{lst:sql-rootfiles}.

	\begin{description}
		\item[File Identifier (\texttt{fid})] 
		Identifier of the file, unique within the index database.

		\item[Device of Origin (\texttt{sid})]
		Identifier of the Timepix device, which acquired all data stored within this file.

		\item[File Path (\texttt{path})] 
		Absolute path to the file in the server's file system.

		\item[Date of Addition (\texttt{date\_added})]
		Date and time, when the file was added to the database.

		\item[Covered Time Interval (\texttt{start\_time}, \texttt{end\_time})]
		Minimum and maximum start time of the Timepix frames stored within this file.

		\item[Statistics (\texttt{count\_frames}, \texttt{count\_entries})]
		The total number of frames and clusters stored in within this file.

		\item[Validation Data (\texttt{checksum}, \texttt{date\_checked})]
		SHA1 checksum of the file and the latest date and time, when the file was validated against it to prevent data corruption.
	\end{description}

	\item[Frame]
	Frame represents a single event of data acquisition from a Timepix device. Every frame is stored in some file (and a file can contain multiple frames). For full definition of the SQL table, see Listing \ref{lst:sql-frames}.

	\begin{description}
		\item[Frame Identifier (\texttt{frid})] 
		Identifier of the frame, unique within the index database.

		\item[File Identifier (\texttt{fid})]
		Identifier of the file, in which the frame is stored.

		\item[Sensor Identifier (\texttt{sid})] 
		Identifier of the device, which captured this frame (must match \texttt{sid} of the file).

		\item[Start Time (\texttt{start\_time})] 
		Start time of the acquisition.

		\item[Acquisition Time (\texttt{acquisition\_time})] 
		Duration of the acquisition.

		\item[Data Addresses (\texttt{dsc\_entry}, \texttt{clstr\_first\_entry})]
		Index values pointing directly to entries within the ROOT file's internal structure, where the frame data is stored.

		\item[Statistics (\texttt{occupancy}, \texttt{clstr1\_count}, \dots, \texttt{clstr6\_count})]
		Total number of non-zero pixels in the frame, and numbers of clusters of different types in the frame.
	\end{description}
\end{description}


\subsection{Performance Optimization}
By the definition, it follows that our index database will help deterministically resolve all time-based queries, even in situations when frames are stored in an undefined directory structure. Apart from this optimization, the database will also provide file validation primitives to ensure that any corrupted files are discovered as soon as possible. But there is one more significant performance optimization we have so far neglected to mention.

When retrieving frames by the time and device of origin, we can use the predefined naming patterns to obtain a path in the file system. In case the path points to a directory, we can consult the index database to quickly scan for a file containing the information we need. Still, we are left with an unoptimized task since in order to retrieve the specific frame (or frames) we are looking for, we will have to scan the entire file, which may be several gigabytes in size.

This issue may be in part resolved by sorting all frames in our data files consistently by their start time, allowing us to use a binary search algorithm instead of regular one, thus reducing the complexity of the operation from linear to logarithmic. But we can still do better. Since we already perform lookups in the index database, we can us them to retrieve data addresses, which will point us to specific locations in the file. This way, no search will be needed at all.

Recall that ROOT files contain two trees of interest, the \texttt{dscData} tree with information about detector configuration, and the \texttt{clusterFile} tree, which contains concatenated lists of clusters from every frame. There is only one entry per frame in the \texttt{dscData} tree, whereas the \texttt{clusterFile} tree may contain anywhere from zero to hundreds of thousands of entries in a single frame. Entries belonging to the same frame can be identified by having equal value of the \texttt{Start\_time} leaf. If we agree to sort all entries in both trees by this leaf value, the \texttt{clusterFile} entries will consequently form continuous bulks of data corresponding to individual frames. This means that once we discover the bulk belonging to the frame we want, we only need to read entries as long as the start time remains the same (or we reach the last entry). For every frame, we can then store the index of the corresponding \texttt{dscData} entry and the index of the first \texttt{clusterFile} entry in the bulk to achieve constant-time search operation, as illustrated in Figure \ref{fig:db-index-optimization}. This significant benefit comes at the price of increased complexity of the insert operation due to additional sorting of entries in our files, and slightly increased space occupied by the index database because of stored entry indices.

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}[
    >=stealth,
    node distance=3cm,
    database/.style={
      cylinder,
      shape border rotate=90,
      aspect=0.25,
      minimum width=2.5cm,
      minimum height=1.5cm,
      draw
    },
    mymat/.style={
      matrix of math nodes,
      text height=2.5ex,
      text depth=0.75ex,
      text width=3.25ex,
      align=center,
      column sep=-\pgflinewidth,
      row sep=-\pgflinewidth
    }]
    % The DB.
    \node[database] (db1) at (0,0) {Index DB};

    % The array.
	\matrix[mymat,anchor=west,row 2/.style={nodes=draw},below of=db1,row 1 column 4/.style={nodes={draw,fill=gray!30}},row 2 column 4/.style={nodes={draw,fill=gray!30}},row 2 column 5/.style={nodes={draw,fill=gray!30}},row 2 column 6/.style={nodes={draw,fill=gray!30}},row 2 column 7/.style={nodes={draw,fill=gray!30}}] (rootfile)
	{
		0  &  1 &  2 &  \textbf{3} &  4 &  5 &  6 &  7 &  8 &  9 & 10 & \dots \\
		29 & 29 & 31 & 32          & 32 & 32 & 32 & 33 & 37 & 40 & 40 & \dots \\
	};

	% Arrow connecting DB and the array.
	\draw[-{Stealth[scale=2.0]},black!100] (db1) -| node[black,above,font=\scriptsize,anchor=east]{\texttt{clstr\_first\_entry=3}} (rootfile-1-4.north);

	% Array labels on the left.
	\node[black,font=\scriptsize,left of=rootfile-1-1,node distance=5mm,anchor=east]{Entry Index};
	\node[black,font=\scriptsize,left of=rootfile-2-1,node distance=5mm,anchor=east]{Start Time [s]};

	% Label in the bottom.
	\node[black,font=\scriptsize,below of=rootfile,node distance=1cm,anchor=north]{\texttt{clusterFile} Tree};
\end{tikzpicture}

\caption{Illustration of the optimization mechanism provided by the index database.}
\label{fig:db-index-optimization}
\end{center}
\end{figure}


\subsection{Data Aggregation and Metaindexing}
In some cases, users of our database may want to calculate aggregated statistics. Since these types of requests are hard to anticipate and do not constitute a significant portion of all user queries, it is not worth our effort to create separate data structures in order to accelerate their processing. We can, however, make use of data structures we already have in place. For instance, our index database makes a great candidate in particular since it already contains data associated with individual files and frames, and is easily accessible and queriable using SQL. We will therefore include several statistical values, such as count of clusters differentiated by individual cluster types and frame occupancy encoded as number of non-zero pixels.

Users can utilize filtering and aggregation features of SQL to quickly find files and frames in the index database, and if required, analyze their contents more thoroughly.

Lastly, SQL implementations include an analogy to the index mechanism we used to accelerate our access to individual frames within data files. Using their own tree indices built from various columns of data tables, servers can speed up certain queries containing predicates or orderings based on such columns. We can utilize this mechanism to make our data access procedure even faster, in a sense indexing the index database. To see how exactly we make use of this technique, examine Listings \ref{lst:sql-sensors}, \ref{lst:sql-rootfiles} and \ref{lst:sql-frames}.


